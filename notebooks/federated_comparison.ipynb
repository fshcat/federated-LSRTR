{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46f5b424",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fishcat/.local/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "PROJECT_ROOT = os.path.abspath(os.path.dirname(sys.path[0]))\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "from lsr_tensor import *\n",
    "from lsr_bcd_regression import *\n",
    "import torch\n",
    "import torch.nn.functional as f\n",
    "from datasets import *\n",
    "from federated_algos import *\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from federated_tests import *\n",
    "from medmnist import BreastMNIST, VesselMNIST3D\n",
    "import cProfile\n",
    "from torchvision import transforms\n",
    "from copy import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbf4191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic Data\n",
    "shape, ranks, separation_rank = (64, 64), (8, 8), 2\n",
    "loss_fn = f.mse_loss\n",
    "\n",
    "sample_size = 2000\n",
    "val_sample_size = 500\n",
    "clients = 10\n",
    "\n",
    "synth_dataset, synth_val_dataset = synthesize_data(shape, ranks, separation_rank,\\\n",
    "                                                   sample_size, val_sample_size)\n",
    "synth_client_datasets = federate_dataset(synth_dataset, clients)\n",
    "synth_data = (synth_dataset, synth_client_datasets, synth_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109e5406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breast MNIST\n",
    "shape, ranks, separation_rank = (28, 28), (3, 3), 2\n",
    "loss_fn = logistic_loss\n",
    "\n",
    "transform = transforms.Compose([transforms.PILToTensor(), transforms.ConvertImageDtype(torch.float32)])\n",
    "\n",
    "breast_dataset = BreastMNIST(split=\"train\", download=True, transform=transform)\n",
    "breast_client_datasets = federate_dataset(breast_dataset, 10)\n",
    "breast_val_dataset = BreastMNIST(split=\"val\", download=True, transform=transform)\n",
    "breast_data = (breast_dataset, breast_client_datasets, breast_val_dataset)\n",
    "\n",
    "print(\"fraction positive: \", sum(breast_val_dataset[:, 0][1]) / len(breast_val_dataset[:, 0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de348213-7ec9-468f-81be-197093c9b0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vessel MNIST 3D\n",
    "vessel_dataset = VesselMNIST3D(split=\"train\", download=True)\n",
    "vessel_val_dataset = VesselMNIST3D(split=\"val\", download=True)\n",
    "vessel_client_datasets = federate_dataset(vessel_dataset, 10)\n",
    "vessel_data = (vessel_dataset, vessel_client_datasets, vessel_val_dataset)\n",
    "print([len(c) for c in vessel_client_datasets])\n",
    "print(\"fraction positive: \", sum(vessel_val_dataset[:, 0][1]) / len(vessel_val_dataset[:, 0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e44eb5-dffd-4673-8880-8085b564c5e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tuning\n",
    "iters = 100\n",
    "n_workers = 4\n",
    "n_runs = 4\n",
    "\n",
    "path_base = \"../data/vessel_tuning\"\n",
    "dataset, client_datasets, val_dataset = vessel_data\n",
    "loss_fn = logistic_loss\n",
    "aggregator_fn = avg_aggregation\n",
    "\n",
    "shape, ranks, separation_rank = (28, 28, 28), (3, 3, 3), 2\n",
    "steps = 10\n",
    "lr = 0.001\n",
    "momentum = 0.99\n",
    "\n",
    "hypers = {\"max_rounds\": 1, \"max_iter\": iters, \"batch_size\": None, \"lr\": lr, \"momentum\": momentum, \"steps\": steps, \"threshold\": 0.0}\n",
    "lsr_dot_params = (shape, ranks, separation_rank, torch.float64, torch.device('cuda'))\n",
    "\n",
    "print(f\"Training lr {lr} steps {steps} momentum {momentum}\")\n",
    "path = f\"{path_base}/lr{int(lr*1000)}_mom{momentum}_steps{steps}\"\n",
    "args = (lsr_bcd_regression, lsr_dot_params, loss_fn, dataset, val_dataset, hypers, True)\n",
    "\n",
    "run_test(path, n_runs, n_workers, *args)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "700721bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0\n",
      "Run 1\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 43\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data_fn_by_size(size)\n\u001b[1;32m     41\u001b[0m sized_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m names]\n\u001b[0;32m---> 43\u001b[0m run_combined_test(path_base, n_runs, n_trials, n_workers, data_fn, lsr_dot_params, sized_names, methods, arg_list)\n",
      "File \u001b[0;32m~/Federated-LSRTR/federated_tests.py:78\u001b[0m, in \u001b[0;36mrun_combined_test\u001b[0;34m(path, n_runs, n_trials, n_workers, data_fn, tensor_params, names, methods, arg_list, verbose, save_weights)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m method_results[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(method_results[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m][key]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 78\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_weights:\n\u001b[1;32m     79\u001b[0m             torch\u001b[38;5;241m.\u001b[39msave(torch\u001b[38;5;241m.\u001b[39mstack([pinfo[key] \u001b[38;5;28;01mfor\u001b[39;00m pinfo \u001b[38;5;129;01min\u001b[39;00m method_results]), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "# Comparison of federated algos\n",
    "shape, ranks, separation_rank = (64, 64), (8, 8), 2\n",
    "lsr_dot_params = (shape, ranks, separation_rank, torch.float32, torch.device('cpu'))\n",
    "\n",
    "loss_fn = f.mse_loss\n",
    "aggregator_fn = avg_aggregation\n",
    "\n",
    "site_sizes = [20, 100, 200, 300, 400, 500, 1000]\n",
    "iters = 10\n",
    "\n",
    "n_runs = 2\n",
    "n_trials = 2\n",
    "n_workers = 2\n",
    "\n",
    "path_base = \"../data/synth_size_test\"\n",
    "\n",
    "def data_fn_by_size(sample_size, val_sample_size=500, clients=10):\n",
    "    synth_tensor = get_synth_tensor(shape, ranks, separation_rank)\n",
    "    synth_dataset, synth_val_dataset = synthesize_data(synth_tensor, sample_size, val_sample_size)\n",
    "    synth_client_datasets = federate_dataset(synth_dataset, clients)\n",
    "    return (synth_dataset, synth_val_dataset, synth_client_datasets)\n",
    "    \n",
    "methods = [BCD_avg_local, lsr_bcd_regression, BCD_federated_stepwise, BCD_federated_all_factors, BCD_federated_full_iteration, BCD_federated_full_iteration]\n",
    "names = ['local', 'centralized', 'step', 'factors_core', 'one_iter', 'five_iter']\n",
    "\n",
    "gen_hypers = {\"max_rounds\": 1, \"max_iter\": iters, \"batch_size\": None, \"lr\": 0.001, \"momentum\": 0.9, \"steps\": 10, \"threshold\": 0.0}\n",
    "iter_hypers = {\"max_rounds\": iters, \"max_iter\": 1, \"batch_size\": None, \"lr\": 0.001, \"momentum\": 0.9, \"steps\": 10, \"threshold\": 0.0}\n",
    "iter_5_hypers = {\"max_rounds\": iters // 5, \"max_iter\": 5, \"batch_size\": None, \"lr\": 0.001, \"momentum\": 0.9, \"steps\": 10, \"threshold\": 0.0}\n",
    "\n",
    "arg_list = [(gen_hypers, loss_fn, False), # local\n",
    "            (gen_hypers, loss_fn, False), # centralized\n",
    "            (gen_hypers, loss_fn, aggregator_fn, False), # one step\n",
    "            (gen_hypers, loss_fn, aggregator_fn, False), # factors core\n",
    "            (iter_hypers, loss_fn, aggregator_fn, False), # one iter\n",
    "            (iter_5_hypers, loss_fn, aggregator_fn, False)] # five iter\n",
    "\n",
    "for size in site_sizes:\n",
    "    def data_fn():\n",
    "        return data_fn_by_size(size)\n",
    "\n",
    "    sized_names = [f\"{name}_{size}\" for name in names]\n",
    "        \n",
    "    run_combined_test(path_base, n_runs, n_trials, n_workers, data_fn, lsr_dot_params, sized_names, methods, arg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741670f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data from runs\n",
    "path_base = \"../data/synth_size_test\"\n",
    "size = 1000\n",
    "names = [f\"centralized_{size}\", f\"step_{size}\", f\"factors_core_{size}\", f\"one_iter_{size}\", f\"five_iter_{size}\", f\"local_{size}\"]\n",
    "print(names)\n",
    "#names = [f\"lr{int(lr*1000)}_mom{momentum}\" for momentum in [0.0, 0.9, 0.99] for lr in [0.1, 0.003, 0.001]]\n",
    "\n",
    "train_losses, train_std = [], []\n",
    "val_losses, val_std = [], []\n",
    "\n",
    "train_accs, train_acc_std = [], []\n",
    "val_accs, val_acc_std = [], []\n",
    "\n",
    "for name in names:\n",
    "    path = f\"{path_base}/{name}\"\n",
    "    train_losses.append(torch.mean(torch.load(f\"{path}/train_loss\"), axis=0))\n",
    "    train_std.append(torch.std(torch.load(f\"{path}/train_loss\"), axis=0))\n",
    "    \n",
    "    val_losses.append(torch.mean(torch.load(f\"{path}/val_loss\"), axis=0))\n",
    "    val_std.append(torch.std(torch.load(f\"{path}/val_loss\"), axis=0))\n",
    "\n",
    "    #train_accs.append(torch.mean(torch.load(f\"{path}/train_acc\"), axis=0))\n",
    "    #train_acc_std.append(torch.std(torch.load(f\"{path}/train_acc\"), axis=0))\n",
    "    \n",
    "    #val_accs.append(torch.mean(torch.load(f\"{path}/val_acc\"), axis=0))\n",
    "    #val_acc_std.append(torch.std(torch.load(f\"{path}/val_acc\"), axis=0))\n",
    "    \n",
    "print(\"Loaded run data\")\n",
    "print([vl.shape for vl in val_losses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fbc72a-9886-473b-95af-3d03ec879bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data from runs (site size comparison)\n",
    "path_base = \"../data/synth_size_test\"\n",
    "\n",
    "site_sizes = [20, 100, 200, 300, 400, 500, 1000]\n",
    "\n",
    "train_losses, train_std = [], []\n",
    "val_losses, val_std = [], []\n",
    "\n",
    "names = [f\"centralized\", f\"step\", f\"factors_core\", f\"one_iter\", f\"five_iter\", f\"local\"]\n",
    "\n",
    "for name in names:\n",
    "    size_best_train, size_best_train_std = [], []\n",
    "    size_best_val, size_best_val_std = [], []\n",
    "\n",
    "    for size in site_sizes:\n",
    "        full_name = f\"{name}_{size}\"    \n",
    "        path = f\"{path_base}/{full_name}\"\n",
    "        \n",
    "        size_best_train.append(torch.mean(torch.load(f\"{path}/train_loss\").min(dim=1).values))\n",
    "        size_best_train_std.append(torch.std(torch.load(f\"{path}/train_loss\").min(dim=1).values))\n",
    "        \n",
    "        size_best_val.append(torch.mean(torch.load(f\"{path}/val_loss\").min(dim=1).values))\n",
    "        size_best_val_std.append(torch.std(torch.load(f\"{path}/val_loss\").min(dim=1).values))\n",
    "\n",
    "    train_losses.append(torch.stack(size_best_train))\n",
    "    train_std.append(torch.stack(size_best_train_std))\n",
    "    val_losses.append(torch.stack(size_best_val))\n",
    "    val_std.append(torch.stack(size_best_val_std))\n",
    "    \n",
    "print(\"Loaded run data\")\n",
    "print([vl.shape for vl in val_losses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb42283",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"#332288\", \"#117733\", \"#44AA99\", \"#88CCEE\", \"#DDCC77\", \"#CC6677\", \"#AA4499\", \"#882255\", \"#5D070A\"]\n",
    "labels = [\"Unfederated\", \"One step\", \"All factors, core\", \"Full iteration\", \"5 full iterations\", \"Avg Local\"]\n",
    "#labels = names\n",
    "\n",
    "xscales = [1, 1, 1, 1, 5, 1]\n",
    "#xscales = [1, 1, 1, 1, 1, 1]\n",
    "#xscales = [1, 1, 1, 1, 1, 1]\n",
    "\n",
    "metrics = val_losses\n",
    "stds = val_std\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for metric, std, xscale, label, color in zip(metrics, stds, xscales, labels, colors):\n",
    "    plt.plot((np.arange(len(metric)) + 1)*xscale, metric, label=label, color=color, marker='o')\n",
    "    plt.fill_between((np.arange(len(metric)) + 1)*xscale, metric-std, metric+std, color=color, alpha=0.2)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Iteration #\", fontsize=14)\n",
    "plt.ylabel(\"Loss (Mean Squared Error)\", fontsize=14)\n",
    "plt.title(\"Model Validation Loss over Number of Iterations\\n (Synthetic Data)\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2c8104-17f0-46f7-98cb-1c5f531f9feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"#332288\", \"#117733\", \"#44AA99\", \"#88CCEE\", \"#DDCC77\", \"#CC6677\", \"#AA4499\", \"#882255\", \"#5D070A\"]\n",
    "labels = [\"Unfederated\", \"One step\", \"All factors, core\", \"Full iteration\", \"5 full iterations\", \"Avg Local\"]\n",
    "#labels = names\n",
    "\n",
    "xscales = [1, 1, 1, 1, 1, 1]\n",
    "#xscales = [1, 1, 1, 1, 1, 1]\n",
    "#xscales = [1, 1, 1, 1, 1, 1]\n",
    "\n",
    "metrics = val_losses\n",
    "stds = val_std\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for metric, std, xscale, label, color in zip(metrics, stds, xscales, labels, colors):\n",
    "    plt.plot([20, 100, 200, 300, 400, 500, 1000], metric, label=label, color=color, marker='o')\n",
    "    plt.fill_between([20, 100, 200, 300, 400, 500, 1000], metric-std, metric+std, color=color, alpha=0.2)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Amt of Data at Each Site\", fontsize=14)\n",
    "plt.ylabel(\"Loss (Mean Squared Error)\", fontsize=14)\n",
    "plt.title(\"Minimum Validation Loss over Site Sample Size\\n (Synthetic Data)\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71af6fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Testing...\")\n",
    "\n",
    "hypers = {\"max_rounds\": 1, \"max_iter\": 100, \"batch_size\": None, \"lr\": 0.001, \"momentum\": 0.9, \"steps\": 10, \"threshold\": 0.0}\n",
    "aggregator_fn = logistic_loss\n",
    "lsr_dot_params = (shape, (3, 3, 3), 2, torch.float64, torch.device('cuda'))\n",
    "\n",
    "init_lsr_dot = LSR_tensor_dot(*lsr_dot_params)\n",
    "lsr_bcd_regression(init_lsr_dot, loss_fn, vessel_dataset, vessel_val_dataset,\\\n",
    "                       hypers, True, True)\n",
    "\n",
    "print(\"Finished without errors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de758f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance testing\n",
    "print(\"Performance Testing...\")\n",
    "hypers = {\"max_rounds\": 1, \"max_iter\": 100, \"batch_size\": None, \"lr\": 0.005, \"momentum\": 0.9, \"steps\": 10, \"threshold\": 0.0}\n",
    "aggregator_fn = avg_aggregation\n",
    "lsr_dot_params = (shape, ranks, separation_rank, torch.float32, torch.device('cuda'))\n",
    "init_lsr_dot = LSR_tensor_dot(*lsr_dot_params)\n",
    "cProfile.run(\"BCD_federated_stepwise(init_lsr_dot, synth_client_datasets, synth_val_dataset,\\\n",
    "              hypers, loss_fn, aggregator_fn, False)\", sort='tottime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a44b90a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         7896414 function calls (7379812 primitive calls) in 15.423 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "    25500    2.034    0.000    2.034    0.000 {method 'run_backward' of 'torch._C._EngineBase' objects}\n",
      "   185302    1.121    0.000    1.121    0.000 {built-in method torch.reshape}\n",
      "    31012    0.758    0.000    0.758    0.000 {built-in method torch._C._nn.mse_loss}\n",
      "    60060    0.564    0.000    0.564    0.000 {built-in method torch.matmul}\n",
      "    56275    0.564    0.000    0.564    0.000 {built-in method torch._ops.profiler._record_function_enter_new}\n",
      "476764/88376    0.427    0.000    4.360    0.000 __init__.py:202(wrapped_backend_method)\n",
      "   214800    0.390    0.000    0.390    0.000 dataset.py:196(<genexpr>)\n",
      "    33052    0.378    0.000    0.378    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "    22950    0.334    0.000    0.334    0.000 {method 'mul_' of 'torch._C._TensorBase' objects}\n",
      "    10458    0.318    0.000    0.318    0.000 {built-in method torch.stack}\n",
      "    33052    0.256    0.000    2.152    0.000 generalised_inner_product.py:8(inner)\n",
      "     1600    0.244    0.000    6.865    0.004 federated_algos.py:34(client_update_factor)\n",
      "    56275    0.235    0.000    0.235    0.000 {built-in method torch._ops.profiler.}\n",
      "    25500    0.235    0.000    2.196    0.000 optimizer.py:265(wrapper)\n",
      "     7772    0.235    0.000    1.700    0.000 lsr_tensor.py:65(expand_to_tensor)\n",
      "    60646    0.219    0.000    0.219    0.000 {built-in method torch.moveaxis}\n",
      "    22440    0.208    0.000    0.208    0.000 {built-in method torch.cat}\n",
      "    67634    0.208    0.000    0.208    0.000 {built-in method torch.squeeze}\n",
      "    25500    0.182    0.000    0.692    0.000 optimizer.py:435(zero_grad)\n",
      "    48450    0.180    0.000    0.180    0.000 {method 'add_' of 'torch._C._TensorBase' objects}\n",
      "    56275    0.174    0.000    0.448    0.000 profiler.py:495(__exit__)\n",
      "   613711    0.161    0.000    0.192    0.000 {built-in method builtins.getattr}\n",
      "     2040    0.153    0.000    0.784    0.000 lsr_tensor.py:86(bcd_factor_update_x)\n",
      "    20400    0.143    0.000    2.297    0.000 lsr_tensor.py:120(bcd_factor_forward)\n",
      "      572    0.140    0.000    3.484    0.006 federated_algos.py:107(get_full_loss)\n",
      "      510    0.134    0.000    0.378    0.001 lsr_tensor.py:100(bcd_core_update_x)\n",
      "    25500    0.133    0.000    0.160    0.000 sgd.py:37(_init_group)\n",
      "    20404    0.127    0.000    0.127    0.000 {built-in method torch.ones}\n",
      "    27008    0.125    0.000    1.051    0.000 n_mode_product.py:5(mode_dot)\n",
      "     1020    0.124    0.000    0.124    0.000 {built-in method torch.kron}\n",
      "       11    0.111    0.010    5.781    0.526 lsr_bcd_regression.py:6(lsr_bcd_regression)\n",
      "    25500    0.111    0.000    0.111    0.000 {built-in method torch.ones_like}\n",
      "    31012    0.109    0.000    1.160    0.000 functional.py:3267(mse_loss)\n",
      "    33052    0.105    0.000    0.534    0.000 fromnumeric.py:69(_wrapreduction)\n",
      "474641/400963    0.104    0.000    0.182    0.000 {built-in method builtins.len}\n",
      "    56275    0.103    0.000    0.222    0.000 profiler.py:482(__init__)\n",
      "   142259    0.103    0.000    0.107    0.000 module.py:1601(__getattr__)\n",
      "    31012    0.102    0.000    0.102    0.000 {built-in method torch.broadcast_tensors}\n",
      "    25500    0.098    0.000    2.419    0.000 __init__.py:106(backward)\n",
      "    56275    0.097    0.000    0.699    0.000 profiler.py:491(__enter__)\n",
      "    13504    0.096    0.000    1.412    0.000 n_mode_product.py:81(multi_mode_dot)\n",
      "    31012    0.093    0.000    0.229    0.000 functional.py:45(broadcast_tensors)\n",
      "    25500    0.093    0.000    0.228    0.000 __init__.py:50(_make_grads)\n",
      "    25500    0.093    0.000    1.220    0.000 optimizer.py:29(_use_grad)\n",
      "    60060    0.087    0.000    0.651    0.000 pytorch_backend.py:109(dot)\n",
      "    25500    0.087    0.000    1.043    0.000 sgd.py:56(step)\n",
      "     1768    0.086    0.000    0.230    0.000 lsr_tensor.py:126(orthonorm_factor)\n",
      "     1772    0.083    0.000    0.083    0.000 {built-in method torch._C._linalg.linalg_qr}\n",
      "    71356    0.081    0.000    0.081    0.000 {method 'to' of 'torch._C._TensorBase' objects}\n",
      "    25500    0.079    0.000    0.628    0.000 sgd.py:233(_single_tensor_sgd)\n",
      "    71600    0.079    0.000    0.469    0.000 dataset.py:195(__getitem__)\n",
      "    71628    0.075    0.000    0.139    0.000 container.py:586(_get_abs_string_index)\n",
      "362355/340906    0.075    0.000    0.118    0.000 {built-in method builtins.isinstance}\n",
      "63908/61868    0.068    0.000    0.358    0.000 container.py:603(__getitem__)\n",
      "    66104    0.067    0.000    0.067    0.000 pytorch_backend.py:56(shape)\n",
      "      400    0.064    0.000    1.704    0.004 federated_algos.py:5(client_update_core)\n",
      "     5275    0.061    0.000    1.050    0.000 dataloader.py:675(_next_data)\n",
      "    29048    0.061    0.000    0.351    0.000 base.py:39(unfold)\n",
      "    62832    0.060    0.000    0.090    0.000 grad_mode.py:149(__init__)\n",
      "    25500    0.060    0.000    2.486    0.000 _tensor.py:428(backward)\n",
      "56276/56275    0.059    0.000    0.111    0.000 typing.py:255(inner)\n",
      "    27008    0.059    0.000    0.285    0.000 base.py:56(fold)\n",
      "    25500    0.057    0.000    0.103    0.000 optimizer.py:64(_default_to_fused_or_foreach)\n",
      "   568297    0.057    0.000    0.057    0.000 {method 'get' of 'dict' objects}\n",
      "    35792    0.054    0.000    0.113    0.000 container.py:281(_get_abs_string_index)\n",
      "    25500    0.050    0.000    0.785    0.000 sgd.py:187(sgd)\n",
      "      827    0.049    0.000    0.214    0.000 lsr_tensor.py:21(init_params)\n",
      "15399/5133    0.048    0.000    0.450    0.000 collate.py:87(collate)\n",
      "    33052    0.048    0.000    0.581    0.000 fromnumeric.py:2970(prod)\n",
      "     6680    0.047    0.000    0.047    0.000 {built-in method torch.clone}\n",
      "    33052    0.047    0.000    0.628    0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}\n",
      "    24636    0.044    0.000    0.135    0.000 module.py:1617(__setattr__)\n",
      "     5072    0.041    0.000    0.041    0.000 {built-in method torch.eye}\n",
      "    35792    0.040    0.000    0.159    0.000 container.py:290(__getitem__)\n",
      "    62314    0.038    0.000    0.038    0.000 {method 'size' of 'torch._C._TensorBase' objects}\n",
      "    56275    0.038    0.000    0.602    0.000 _ops.py:497(__call__)\n",
      "    13504    0.036    0.000    0.186    0.000 {built-in method builtins.sorted}\n",
      "    56276    0.035    0.000    0.051    0.000 typing.py:329(__hash__)\n",
      "     2550    0.033    0.000    0.050    0.000 optimizer.py:489(add_param_group)\n",
      "    51000    0.033    0.000    0.033    0.000 optimizer.py:72(<genexpr>)\n",
      "     5275    0.032    0.000    1.254    0.000 dataloader.py:628(__next__)\n",
      "     4328    0.032    0.000    0.038    0.000 module.py:437(__init__)\n",
      "    56275    0.031    0.000    0.266    0.000 _ops.py:286(__call__)\n",
      "     5133    0.030    0.000    0.501    0.000 fetch.py:51(<listcomp>)\n",
      "    36073    0.030    0.000    0.030    0.000 {method 'format' of 'str' objects}\n",
      "    33052    0.030    0.000    0.666    0.000 <__array_function__ internals>:177(prod)\n",
      "    94248    0.029    0.000    0.029    0.000 {built-in method torch.is_grad_enabled}\n",
      "     5876    0.029    0.000    5.364    0.001 _contextlib.py:112(decorate_context)\n",
      "   180292    0.029    0.000    0.029    0.000 container.py:625(__len__)\n",
      "    25500    0.028    0.000    0.028    0.000 {built-in method torch._C._are_functorch_transforms_active}\n",
      "   105060    0.027    0.000    0.045    0.000 _tensor.py:942(__hash__)\n",
      "    15300    0.026    0.000    0.072    0.000 module.py:2045(_named_members)\n",
      "     9799    0.025    0.000    0.052    0.000 module.py:539(register_parameter)\n",
      "    25500    0.024    0.000    0.392    0.000 base.py:5(tensor_to_vec)\n",
      "33150/12750    0.024    0.000    0.027    0.000 module.py:2224(named_modules)\n",
      "     4590    0.024    0.000    0.098    0.000 base.py:82(partial_unfold)\n",
      "     2550    0.022    0.000    0.176    0.000 optimizer.py:169(__init__)\n",
      "    73538    0.022    0.000    0.029    0.000 container.py:311(__len__)\n",
      "     7599    0.022    0.000    0.022    0.000 {built-in method _make_subclass}\n",
      "    60098    0.021    0.000    0.021    0.000 {method 'dim' of 'torch._C._TensorBase' objects}\n",
      "      826    0.021    0.000    0.393    0.000 lsr_tensor.py:49(copy)\n",
      "    33052    0.021    0.000    0.021    0.000 fromnumeric.py:70(<dictcomp>)\n",
      "    47484    0.020    0.000    0.036    0.000 {built-in method builtins.hasattr}\n",
      "    30633    0.019    0.000    0.062    0.000 {built-in method builtins.all}\n",
      "    14524    0.019    0.000    0.027    0.000 container.py:628(__iter__)\n",
      "    54016    0.018    0.000    0.037    0.000 pytorch_backend.py:60(ndim)\n",
      "   122762    0.018    0.000    0.018    0.000 {method 'values' of 'collections.OrderedDict' objects}\n",
      "     5100    0.018    0.000    0.411    0.000 lsr_tensor.py:114(bcd_core_forward)\n",
      "   105060    0.018    0.000    0.018    0.000 {built-in method builtins.id}\n",
      "     5512    0.018    0.000    1.811    0.000 lsr_tensor.py:81(forward)\n",
      "    43572    0.018    0.000    0.154    0.000 container.py:629(<genexpr>)\n",
      "    21449    0.017    0.000    0.025    0.000 parameter.py:8(__instancecheck__)\n",
      "      300    0.017    0.000    5.339    0.018 federated_algos.py:63(client_update_factors)\n",
      "    10266    0.016    0.000    0.331    0.000 collate.py:153(collate_tensor_fn)\n",
      "    62832    0.016    0.000    0.016    0.000 {built-in method torch._C._set_grad_enabled}\n",
      "    31012    0.016    0.000    0.024    0.000 _VF.py:26(__getattr__)\n",
      "     8740    0.016    0.000    0.143    0.000 container.py:614(__setitem__)\n",
      "    56278    0.016    0.000    0.016    0.000 {built-in method builtins.hash}\n",
      "   107420    0.015    0.000    0.015    0.000 {built-in method _operator.index}\n",
      "   108303    0.015    0.000    0.015    0.000 {method 'append' of 'list' objects}\n",
      "   113191    0.015    0.000    0.015    0.000 _jit_internal.py:1102(is_scripting)\n",
      "    31012    0.015    0.000    0.015    0.000 _reduction.py:7(get_enum)\n",
      "     5326    0.015    0.000    0.016    0.000 sampler.py:241(__iter__)\n",
      "     2550    0.015    0.000    0.015    0.000 {built-in method torch.unsqueeze}\n",
      "    25500    0.014    0.000    0.014    0.000 {method 'numel' of 'torch._C._TensorBase' objects}\n",
      "    71600    0.014    0.000    0.018    0.000 collate.py:137(<genexpr>)\n",
      "     6082    0.014    0.000    0.027    0.000 _tensor.py:904(__len__)\n",
      "     3060    0.013    0.000    0.238    0.000 _kronecker.py:8(kronecker)\n",
      "     5916    0.013    0.000    0.014    0.000 grad_mode.py:48(__init__)\n",
      "     1768    0.012    0.000    0.012    0.000 {built-in method torch.sign}\n",
      "     6944    0.012    0.000    0.012    0.000 {built-in method torch._C._log_api_usage_once}\n",
      "        1    0.012    0.012    2.546    2.546 federated_algos.py:144(BCD_federated_stepwise)\n",
      "    25500    0.012    0.000    0.012    0.000 __init__.py:98(_tensor_or_tensors_to_tuple)\n",
      "    31012    0.011    0.000    0.011    0.000 {built-in method torch._C._has_torch_function_variadic}\n",
      "     2550    0.011    0.000    0.187    0.000 sgd.py:11(__init__)\n",
      "    31012    0.010    0.000    0.010    0.000 {built-in method torch._C._has_torch_function}\n",
      "    10977    0.010    0.000    0.017    0.000 {built-in method _abc._abc_instancecheck}\n",
      "     5133    0.010    0.000    0.968    0.000 fetch.py:46(fetch)\n",
      "     2550    0.010    0.000    0.010    0.000 {method 'detach' of 'torch._C._TensorBase' objects}\n",
      "     6082    0.009    0.000    0.009    0.000 {built-in method torch._C._get_tracing_state}\n",
      "    31692    0.009    0.000    0.009    0.000 {built-in method torch._C._has_torch_function_unary}\n",
      "    15300    0.009    0.000    0.080    0.000 module.py:2084(named_parameters)\n",
      "    56275    0.009    0.000    0.009    0.000 __init__.py:89(annotate)\n",
      "    15311    0.008    0.000    0.088    0.000 module.py:2059(parameters)\n",
      "     5916    0.008    0.000    0.020    0.000 grad_mode.py:57(__exit__)\n",
      "     5348    0.008    0.000    0.122    0.000 container.py:639(append)\n",
      "     8746    0.008    0.000    0.075    0.000 {built-in method builtins.setattr}\n",
      "    33052    0.008    0.000    0.008    0.000 fromnumeric.py:2965(_prod_dispatcher)\n",
      "    10826    0.008    0.000    0.008    0.000 module.py:1618(remove_from)\n",
      "      830    0.008    0.000    0.008    0.000 {built-in method torch.zeros}\n",
      "     5133    0.008    0.000    0.348    0.000 collate.py:142(<listcomp>)\n",
      "     8282    0.008    0.000    0.012    0.000 container.py:315(__iter__)\n",
      "      827    0.007    0.000    0.249    0.000 lsr_tensor.py:8(__init__)\n",
      "     5916    0.007    0.000    0.016    0.000 grad_mode.py:53(__enter__)\n",
      "    27008    0.007    0.000    0.007    0.000 {method 'pop' of 'list' objects}\n",
      "     5876    0.006    0.000    0.020    0.000 _contextlib.py:141(clone)\n",
      "28376/28325    0.006    0.000    0.009    0.000 {built-in method builtins.iter}\n",
      "    35602    0.006    0.000    0.006    0.000 {method 'items' of 'dict' objects}\n",
      "    22950    0.006    0.000    0.009    0.000 {method 'add' of 'set' objects}\n",
      "     7599    0.006    0.000    0.027    0.000 parameter.py:30(__new__)\n",
      "     4590    0.006    0.000    0.006    0.000 base.py:113(<listcomp>)\n",
      "     5133    0.005    0.000    0.455    0.000 collate.py:204(default_collate)\n",
      "     2674    0.005    0.000    0.039    0.000 container.py:580(__init__)\n",
      "    27008    0.005    0.000    0.005    0.000 {method 'insert' of 'list' objects}\n",
      "5217/5137    0.005    0.000    0.006    0.000 {built-in method _abc._abc_subclasscheck}\n",
      "    27008    0.005    0.000    0.005    0.000 n_mode_product.py:124(<lambda>)\n",
      "    10977    0.004    0.000    0.022    0.000 abc.py:96(__instancecheck__)\n",
      "    10200    0.004    0.000    0.006    0.000 module.py:2113(<lambda>)\n",
      "10459/10357    0.004    0.000    0.035    0.000 {built-in method builtins.next}\n",
      "        1    0.004    0.004    2.425    2.425 federated_algos.py:198(BCD_federated_all_factors)\n",
      "    25500    0.004    0.000    0.004    0.000 optimizer.py:249(_optimizer_step_code)\n",
      "     2550    0.004    0.000    0.007    0.000 optimizer.py:291(_patch_step_function)\n",
      "    21449    0.004    0.000    0.004    0.000 {function _ParameterMeta.__instancecheck__ at 0x7f9ac3a03310}\n",
      "     2550    0.004    0.000    0.048    0.000 base.py:150(partial_tensor_to_vec)\n",
      "      162    0.004    0.000    0.004    0.000 {built-in method torch.mean}\n",
      "     1000    0.003    0.000    0.076    0.000 federated_algos.py:254(<lambda>)\n",
      "    20400    0.003    0.000    0.003    0.000 {method 'setdefault' of 'dict' objects}\n",
      "      120    0.003    0.000    4.459    0.037 federated_algos.py:72(client_update_full)\n",
      "      110    0.003    0.000    0.003    0.000 {built-in method torch._C._linalg.linalg_vector_norm}\n",
      "     1654    0.003    0.000    0.007    0.000 module.py:585(add_module)\n",
      "      193    0.003    0.000    0.012    0.000 dataloader.py:567(__init__)\n",
      "    20400    0.003    0.000    0.003    0.000 {method 'items' of 'collections.OrderedDict' objects}\n",
      "        2    0.003    0.001    4.665    2.333 federated_algos.py:250(BCD_federated_full_iteration)\n",
      "      193    0.003    0.000    0.003    0.000 {method 'random_' of 'torch._C._TensorBase' objects}\n",
      "     5275    0.002    0.000    0.021    0.000 dataloader.py:622(_next_index)\n",
      "      110    0.002    0.000    0.005    0.000 functional.py:1387(norm)\n",
      "     1654    0.002    0.000    0.011    0.000 container.py:377(append)\n",
      "     2040    0.002    0.000    0.012    0.000 _kronecker.py:43(<listcomp>)\n",
      "      500    0.002    0.000    0.005    0.000 dataset.py:295(__getitem__)\n",
      "      500    0.002    0.000    0.038    0.000 federated_algos.py:201(<lambda>)\n",
      "      500    0.002    0.000    0.036    0.000 federated_algos.py:148(<lambda>)\n",
      "    10266    0.002    0.000    0.002    0.000 worker.py:89(get_worker_info)\n",
      "     1020    0.002    0.000    0.002    0.000 {method 'indices' of 'slice' objects}\n",
      "      193    0.001    0.000    0.001    0.000 {built-in method torch.empty}\n",
      "5217/5137    0.001    0.000    0.008    0.000 abc.py:100(__subclasscheck__)\n",
      "     5133    0.001    0.000    0.001    0.000 _collections_abc.py:302(__subclasshook__)\n",
      "       36    0.001    0.000    0.001    0.000 {built-in method builtins.sum}\n",
      "      193    0.001    0.000    0.001    0.000 {method 'item' of 'torch._C._TensorBase' objects}\n",
      "       12    0.001    0.000    0.048    0.004 federated_algos.py:283(<listcomp>)\n",
      "      827    0.001    0.000    0.007    0.000 container.py:276(__init__)\n",
      "     2550    0.001    0.000    0.001    0.000 {method 'isdisjoint' of 'set' objects}\n",
      "       10    0.001    0.000    0.042    0.004 federated_algos.py:235(<listcomp>)\n",
      "      193    0.001    0.000    0.014    0.000 dataloader.py:661(__init__)\n",
      "      160    0.001    0.000    0.010    0.000 federated_algos.py:79(avg_aggregation)\n",
      "       66    0.001    0.000    0.003    0.000 dataloader.py:225(__init__)\n",
      "       10    0.001    0.000    0.041    0.004 federated_algos.py:183(<listcomp>)\n",
      "      572    0.001    0.000    0.001    0.000 {method 'cpu' of 'torch._C._TensorBase' objects}\n",
      "        1    0.001    0.001    5.461    5.461 lsr_bcd_regression.py:137(BCD_avg_local)\n",
      "      193    0.001    0.000    0.002    0.000 dataloader.py:98(_get_distributed_settings)\n",
      "1320/1254    0.001    0.000    0.001    0.000 dataloader.py:418(__setattr__)\n",
      "        1    0.001    0.001   15.423   15.423 federated_tests.py:37(run_combined_trial)\n",
      "       48    0.001    0.000    0.004    0.000 federated_algos.py:275(<listcomp>)\n",
      "      193    0.001    0.000    0.001    0.000 dataloader.py:74(create_fetcher)\n",
      "      193    0.000    0.000    0.001    0.000 sampler.py:75(__iter__)\n",
      "      193    0.000    0.000    0.015    0.000 dataloader.py:383(_get_iterator)\n",
      "      290    0.000    0.000    0.001    0.000 dataset.py:198(__len__)\n",
      "      193    0.000    0.000    0.001    0.000 distributed_c10d.py:683(is_initialized)\n",
      "      198    0.000    0.000    0.000    0.000 typing.py:868(__new__)\n",
      "      193    0.000    0.000    0.015    0.000 dataloader.py:428(__iter__)\n",
      "        4    0.000    0.000    0.013    0.003 federated_algos.py:89(get_client_dataloaders)\n",
      "      193    0.000    0.000    0.000    0.000 distributed_c10d.py:403(WORLD)\n",
      "       40    0.000    0.000    0.003    0.000 federated_algos.py:222(<listcomp>)\n",
      "       72    0.000    0.000    0.000    0.000 os.py:670(__getitem__)\n",
      "      193    0.000    0.000    0.000    0.000 __init__.py:7(is_available)\n",
      "      193    0.000    0.000    0.000    0.000 fetch.py:8(__init__)\n",
      "      452    0.000    0.000    0.000    0.000 dataloader.py:443(_auto_collation)\n",
      "        5    0.000    0.000    0.000    0.000 {built-in method torch.normal}\n",
      "       72    0.000    0.000    0.001    0.000 __init__.py:94(is_available)\n",
      "       66    0.000    0.000    0.000    0.000 {built-in method torch.set_vital}\n",
      "       72    0.000    0.000    0.001    0.000 __init__.py:91(_nvml_based_avail)\n",
      "      140    0.000    0.000    0.001    0.000 dataset.py:300(__len__)\n",
      "       72    0.000    0.000    0.001    0.000 os.py:766(getenv)\n",
      "       72    0.000    0.000    0.000    0.000 _collections_abc.py:657(get)\n",
      "      193    0.000    0.000    0.000    0.000 distributed_c10d.py:323(default_pg)\n",
      "       66    0.000    0.000    0.000    0.000 sampler.py:226(__init__)\n",
      "       72    0.000    0.000    0.000    0.000 os.py:748(encode)\n",
      "      193    0.000    0.000    0.000    0.000 dataloader.py:447(_index_sampler)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method torch._C._get_operation_overload}\n",
      "       12    0.000    0.000    0.000    0.000 federated_algos.py:279(<listcomp>)\n",
      "      198    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x909780}\n",
      "       72    0.000    0.000    0.000    0.000 {built-in method torch._C._cuda_getDeviceCount}\n",
      "        1    0.000    0.000    0.000    0.000 _ops.py:239(__init__)\n",
      "       72    0.000    0.000    0.000    0.000 __init__.py:87(_is_compiled)\n",
      "       66    0.000    0.000    0.000    0.000 dataloader.py:394(multiprocessing_context)\n",
      "        1    0.000    0.000   15.423   15.423 {built-in method builtins.exec}\n",
      "       72    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}\n",
      "       66    0.000    0.000    0.000    0.000 dataloader.py:488(check_worker_number_rationality)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.compile}\n",
      "       66    0.000    0.000    0.000    0.000 sampler.py:72(__init__)\n",
      "        1    0.000    0.000   15.423   15.423 <string>:1(<module>)\n",
      "        1    0.000    0.000    0.000    0.000 _ops.py:447(__getattr__)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method torch._C._get_schema}\n",
      "      2/1    0.000    0.000    0.000    0.000 typing.py:347(__getitem__)\n",
      "        3    0.000    0.000    0.000    0.000 typing.py:120(_type_check)\n",
      "        8    0.000    0.000    0.000    0.000 typing.py:762(__setattr__)\n",
      "        1    0.000    0.000    0.000    0.000 typing.py:659(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 functools.py:34(update_wrapper)\n",
      "        8    0.000    0.000    0.000    0.000 typing.py:646(_is_dunder)\n",
      "        1    0.000    0.000    0.000    0.000 typing.py:219(_remove_dups_flatten)\n",
      "        1    0.000    0.000    0.000    0.000 optimizer.py:262(profile_hook_step)\n",
      "        9    0.000    0.000    0.000    0.000 {method 'startswith' of 'str' objects}\n",
      "        1    0.000    0.000    0.000    0.000 typing.py:496(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 typing_extensions.py:121(_collect_type_vars)\n",
      "        1    0.000    0.000    0.000    0.000 functools.py:64(wraps)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'split' of 'str' objects}\n",
      "        1    0.000    0.000    0.000    0.000 lsr_bcd_regression.py:156(<listcomp>)\n",
      "        3    0.000    0.000    0.000    0.000 typing.py:358(<genexpr>)\n",
      "        2    0.000    0.000    0.000    0.000 typing_extensions.py:117(_should_collect_from_parameters)\n",
      "        2    0.000    0.000    0.000    0.000 typing.py:524(__eq__)\n",
      "        1    0.000    0.000    0.000    0.000 lsr_bcd_regression.py:157(<listcomp>)\n",
      "        3    0.000    0.000    0.000    0.000 typing.py:669(<genexpr>)\n",
      "        2    0.000    0.000    0.000    0.000 typing.py:532(__hash__)\n",
      "        5    0.000    0.000    0.000    0.000 {method 'endswith' of 'str' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Comparison of federated algos\n",
    "shape, ranks, separation_rank = (64, 64), (8, 8), 2\n",
    "lsr_dot_params = (shape, ranks, separation_rank, torch.float32, torch.device('cpu'))\n",
    "\n",
    "loss_fn = f.mse_loss\n",
    "aggregator_fn = avg_aggregation\n",
    "\n",
    "iters = 10\n",
    "\n",
    "path_base = \"../data/synth_size_test\"\n",
    "\n",
    "def data_fn_by_size(sample_size, val_sample_size=500, clients=10):\n",
    "    synth_tensor = get_synth_tensor(shape, ranks, separation_rank)\n",
    "    synth_dataset, synth_val_dataset = synthesize_data(synth_tensor, sample_size, val_sample_size)\n",
    "    synth_client_datasets = federate_dataset(synth_dataset, clients)\n",
    "    return (synth_dataset, synth_val_dataset, synth_client_datasets)\n",
    "    \n",
    "data = data_fn_by_size(sample_size=100)\n",
    "\n",
    "methods = [BCD_avg_local, lsr_bcd_regression, BCD_federated_stepwise, BCD_federated_all_factors, BCD_federated_full_iteration, BCD_federated_full_iteration]\n",
    "names = ['local', 'centralized', 'step', 'factors_core', 'one_iter', 'five_iter']\n",
    "\n",
    "gen_hypers = {\"max_rounds\": 1, \"max_iter\": iters, \"batch_size\": None, \"lr\": 0.001, \"momentum\": 0.9, \"steps\": 10, \"threshold\": 0.0}\n",
    "iter_hypers = {\"max_rounds\": iters, \"max_iter\": 1, \"batch_size\": None, \"lr\": 0.001, \"momentum\": 0.9, \"steps\": 10, \"threshold\": 0.0}\n",
    "iter_5_hypers = {\"max_rounds\": iters // 5, \"max_iter\": 5, \"batch_size\": None, \"lr\": 0.001, \"momentum\": 0.9, \"steps\": 10, \"threshold\": 0.0}\n",
    "\n",
    "arg_list = [(gen_hypers, loss_fn, False), # local\n",
    "            (gen_hypers, loss_fn, False), # centralized\n",
    "            (gen_hypers, loss_fn, aggregator_fn, False), # one step\n",
    "            (gen_hypers, loss_fn, aggregator_fn, False), # factors core\n",
    "            (iter_hypers, loss_fn, aggregator_fn, False), # one iter\n",
    "            (iter_5_hypers, loss_fn, aggregator_fn, False)] # five iter\n",
    "\n",
    "cProfile.run(\"run_combined_trial(methods, lsr_dot_params, data, arg_list, False)\", sort='tottime')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
