{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46f5b424",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fishcat/.local/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "PROJECT_ROOT = os.path.abspath(os.path.dirname(sys.path[0]))\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "from lsr_tensor import *\n",
    "from lsr_bcd_regression import *\n",
    "import torch\n",
    "import torch.nn.functional as f\n",
    "from datasets import *\n",
    "from federated_algos import *\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from federated_tests import *\n",
    "from medmnist import BreastMNIST, VesselMNIST3D\n",
    "import cProfile\n",
    "from torchvision import transforms\n",
    "from copy import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbf4191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic Data\n",
    "shape, ranks, separation_rank = (64, 64), (8, 8), 2\n",
    "loss_fn = f.mse_loss\n",
    "\n",
    "sample_size = 2000\n",
    "val_sample_size = 500\n",
    "clients = 10\n",
    "\n",
    "synth_dataset, synth_val_dataset = synthesize_data(shape, ranks, separation_rank,\\\n",
    "                                                   sample_size, val_sample_size)\n",
    "synth_client_datasets = federate_dataset(synth_dataset, clients)\n",
    "synth_data = (synth_dataset, synth_client_datasets, synth_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109e5406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breast MNIST\n",
    "shape, ranks, separation_rank = (28, 28), (3, 3), 2\n",
    "loss_fn = logistic_loss\n",
    "\n",
    "transform = transforms.Compose([transforms.PILToTensor(), transforms.ConvertImageDtype(torch.float32)])\n",
    "\n",
    "breast_dataset = BreastMNIST(split=\"train\", download=True, transform=transform)\n",
    "breast_client_datasets = federate_dataset(breast_dataset, 10)\n",
    "breast_val_dataset = BreastMNIST(split=\"val\", download=True, transform=transform)\n",
    "breast_data = (breast_dataset, breast_client_datasets, breast_val_dataset)\n",
    "\n",
    "print(\"fraction positive: \", sum(breast_val_dataset[:, 0][1]) / len(breast_val_dataset[:, 0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de348213-7ec9-468f-81be-197093c9b0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vessel MNIST 3D\n",
    "vessel_dataset = VesselMNIST3D(split=\"train\", download=True)\n",
    "vessel_val_dataset = VesselMNIST3D(split=\"val\", download=True)\n",
    "vessel_client_datasets = federate_dataset(vessel_dataset, 10)\n",
    "vessel_data = (vessel_dataset, vessel_client_datasets, vessel_val_dataset)\n",
    "print([len(c) for c in vessel_client_datasets])\n",
    "print(\"fraction positive: \", sum(vessel_val_dataset[:, 0][1]) / len(vessel_val_dataset[:, 0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e44eb5-dffd-4673-8880-8085b564c5e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tuning\n",
    "iters = 100\n",
    "n_workers = 4\n",
    "n_runs = 4\n",
    "\n",
    "path_base = \"../data/vessel_tuning\"\n",
    "dataset, client_datasets, val_dataset = vessel_data\n",
    "loss_fn = logistic_loss\n",
    "aggregator_fn = avg_aggregation\n",
    "\n",
    "shape, ranks, separation_rank = (28, 28, 28), (3, 3, 3), 2\n",
    "steps = 10\n",
    "lr = 0.001\n",
    "momentum = 0.99\n",
    "\n",
    "hypers = {\"max_rounds\": 1, \"max_iter\": iters, \"batch_size\": None, \"lr\": lr, \"momentum\": momentum, \"steps\": steps, \"threshold\": 0.0}\n",
    "lsr_dot_params = (shape, ranks, separation_rank, torch.float64, torch.device('cuda'))\n",
    "\n",
    "print(f\"Training lr {lr} steps {steps} momentum {momentum}\")\n",
    "path = f\"{path_base}/lr{int(lr*1000)}_mom{momentum}_steps{steps}\"\n",
    "args = (lsr_bcd_regression, lsr_dot_params, loss_fn, dataset, val_dataset, hypers, True)\n",
    "\n",
    "run_test(path, n_runs, n_workers, *args)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "700721bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0\n",
      "Run 1\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 43\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data_fn_by_size(size)\n\u001b[1;32m     41\u001b[0m sized_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m names]\n\u001b[0;32m---> 43\u001b[0m run_combined_test(path_base, n_runs, n_trials, n_workers, data_fn, lsr_dot_params, sized_names, methods, arg_list)\n",
      "File \u001b[0;32m~/Federated-LSRTR/federated_tests.py:78\u001b[0m, in \u001b[0;36mrun_combined_test\u001b[0;34m(path, n_runs, n_trials, n_workers, data_fn, tensor_params, names, methods, arg_list, verbose, save_weights)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m method_results[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(method_results[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m][key]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 78\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_weights:\n\u001b[1;32m     79\u001b[0m             torch\u001b[38;5;241m.\u001b[39msave(torch\u001b[38;5;241m.\u001b[39mstack([pinfo[key] \u001b[38;5;28;01mfor\u001b[39;00m pinfo \u001b[38;5;129;01min\u001b[39;00m method_results]), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "# Comparison of federated algos\n",
    "shape, ranks, separation_rank = (64, 64), (8, 8), 2\n",
    "lsr_dot_params = (shape, ranks, separation_rank, torch.float32, torch.device('cpu'))\n",
    "\n",
    "loss_fn = f.mse_loss\n",
    "aggregator_fn = avg_aggregation\n",
    "\n",
    "site_sizes = [20, 100, 200, 300, 400, 500, 1000]\n",
    "iters = 10\n",
    "\n",
    "n_runs = 2\n",
    "n_trials = 2\n",
    "n_workers = 2\n",
    "\n",
    "path_base = \"../data/synth_size_test\"\n",
    "\n",
    "def data_fn_by_size(sample_size, val_sample_size=500, clients=10):\n",
    "    synth_tensor = get_synth_tensor(shape, ranks, separation_rank)\n",
    "    synth_dataset, synth_val_dataset = synthesize_data(synth_tensor, sample_size, val_sample_size)\n",
    "    synth_client_datasets = federate_dataset(synth_dataset, clients)\n",
    "    return (synth_dataset, synth_val_dataset, synth_client_datasets)\n",
    "    \n",
    "methods = [BCD_avg_local, lsr_bcd_regression, BCD_federated_stepwise, BCD_federated_all_factors, BCD_federated_full_iteration, BCD_federated_full_iteration]\n",
    "names = ['local', 'centralized', 'step', 'factors_core', 'one_iter', 'five_iter']\n",
    "\n",
    "gen_hypers = {\"max_rounds\": 1, \"max_iter\": iters, \"batch_size\": None, \"lr\": 0.001, \"momentum\": 0.9, \"steps\": 10, \"threshold\": 0.0}\n",
    "iter_hypers = {\"max_rounds\": iters, \"max_iter\": 1, \"batch_size\": None, \"lr\": 0.001, \"momentum\": 0.9, \"steps\": 10, \"threshold\": 0.0}\n",
    "iter_5_hypers = {\"max_rounds\": iters // 5, \"max_iter\": 5, \"batch_size\": None, \"lr\": 0.001, \"momentum\": 0.9, \"steps\": 10, \"threshold\": 0.0}\n",
    "\n",
    "arg_list = [(gen_hypers, loss_fn, False), # local\n",
    "            (gen_hypers, loss_fn, False), # centralized\n",
    "            (gen_hypers, loss_fn, aggregator_fn, False), # one step\n",
    "            (gen_hypers, loss_fn, aggregator_fn, False), # factors core\n",
    "            (iter_hypers, loss_fn, aggregator_fn, False), # one iter\n",
    "            (iter_5_hypers, loss_fn, aggregator_fn, False)] # five iter\n",
    "\n",
    "for size in site_sizes:\n",
    "    def data_fn():\n",
    "        return data_fn_by_size(size)\n",
    "\n",
    "    sized_names = [f\"{name}_{size}\" for name in names]\n",
    "        \n",
    "    run_combined_test(path_base, n_runs, n_trials, n_workers, data_fn, lsr_dot_params, sized_names, methods, arg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741670f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data from runs\n",
    "path_base = \"../data/synth_size_test\"\n",
    "size = 1000\n",
    "names = [f\"centralized_{size}\", f\"step_{size}\", f\"factors_core_{size}\", f\"one_iter_{size}\", f\"five_iter_{size}\", f\"local_{size}\"]\n",
    "print(names)\n",
    "#names = [f\"lr{int(lr*1000)}_mom{momentum}\" for momentum in [0.0, 0.9, 0.99] for lr in [0.1, 0.003, 0.001]]\n",
    "\n",
    "train_losses, train_std = [], []\n",
    "val_losses, val_std = [], []\n",
    "\n",
    "train_accs, train_acc_std = [], []\n",
    "val_accs, val_acc_std = [], []\n",
    "\n",
    "for name in names:\n",
    "    path = f\"{path_base}/{name}\"\n",
    "    train_losses.append(torch.mean(torch.load(f\"{path}/train_loss\"), axis=0))\n",
    "    train_std.append(torch.std(torch.load(f\"{path}/train_loss\"), axis=0))\n",
    "    \n",
    "    val_losses.append(torch.mean(torch.load(f\"{path}/val_loss\"), axis=0))\n",
    "    val_std.append(torch.std(torch.load(f\"{path}/val_loss\"), axis=0))\n",
    "\n",
    "    #train_accs.append(torch.mean(torch.load(f\"{path}/train_acc\"), axis=0))\n",
    "    #train_acc_std.append(torch.std(torch.load(f\"{path}/train_acc\"), axis=0))\n",
    "    \n",
    "    #val_accs.append(torch.mean(torch.load(f\"{path}/val_acc\"), axis=0))\n",
    "    #val_acc_std.append(torch.std(torch.load(f\"{path}/val_acc\"), axis=0))\n",
    "    \n",
    "print(\"Loaded run data\")\n",
    "print([vl.shape for vl in val_losses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fbc72a-9886-473b-95af-3d03ec879bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data from runs (site size comparison)\n",
    "path_base = \"../data/synth_size_test\"\n",
    "\n",
    "site_sizes = [20, 100, 200, 300, 400, 500, 1000]\n",
    "\n",
    "train_losses, train_std = [], []\n",
    "val_losses, val_std = [], []\n",
    "\n",
    "names = [f\"centralized\", f\"step\", f\"factors_core\", f\"one_iter\", f\"five_iter\", f\"local\"]\n",
    "\n",
    "for name in names:\n",
    "    size_best_train, size_best_train_std = [], []\n",
    "    size_best_val, size_best_val_std = [], []\n",
    "\n",
    "    for size in site_sizes:\n",
    "        full_name = f\"{name}_{size}\"    \n",
    "        path = f\"{path_base}/{full_name}\"\n",
    "        \n",
    "        size_best_train.append(torch.mean(torch.load(f\"{path}/train_loss\").min(dim=1).values))\n",
    "        size_best_train_std.append(torch.std(torch.load(f\"{path}/train_loss\").min(dim=1).values))\n",
    "        \n",
    "        size_best_val.append(torch.mean(torch.load(f\"{path}/val_loss\").min(dim=1).values))\n",
    "        size_best_val_std.append(torch.std(torch.load(f\"{path}/val_loss\").min(dim=1).values))\n",
    "\n",
    "    train_losses.append(torch.stack(size_best_train))\n",
    "    train_std.append(torch.stack(size_best_train_std))\n",
    "    val_losses.append(torch.stack(size_best_val))\n",
    "    val_std.append(torch.stack(size_best_val_std))\n",
    "    \n",
    "print(\"Loaded run data\")\n",
    "print([vl.shape for vl in val_losses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb42283",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"#332288\", \"#117733\", \"#44AA99\", \"#88CCEE\", \"#DDCC77\", \"#CC6677\", \"#AA4499\", \"#882255\", \"#5D070A\"]\n",
    "labels = [\"Unfederated\", \"One step\", \"All factors, core\", \"Full iteration\", \"5 full iterations\", \"Avg Local\"]\n",
    "#labels = names\n",
    "\n",
    "xscales = [1, 1, 1, 1, 5, 1]\n",
    "#xscales = [1, 1, 1, 1, 1, 1]\n",
    "#xscales = [1, 1, 1, 1, 1, 1]\n",
    "\n",
    "metrics = val_losses\n",
    "stds = val_std\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for metric, std, xscale, label, color in zip(metrics, stds, xscales, labels, colors):\n",
    "    plt.plot((np.arange(len(metric)) + 1)*xscale, metric, label=label, color=color, marker='o')\n",
    "    plt.fill_between((np.arange(len(metric)) + 1)*xscale, metric-std, metric+std, color=color, alpha=0.2)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Iteration #\", fontsize=14)\n",
    "plt.ylabel(\"Loss (Mean Squared Error)\", fontsize=14)\n",
    "plt.title(\"Model Validation Loss over Number of Iterations\\n (Synthetic Data)\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2c8104-17f0-46f7-98cb-1c5f531f9feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"#332288\", \"#117733\", \"#44AA99\", \"#88CCEE\", \"#DDCC77\", \"#CC6677\", \"#AA4499\", \"#882255\", \"#5D070A\"]\n",
    "labels = [\"Unfederated\", \"One step\", \"All factors, core\", \"Full iteration\", \"5 full iterations\", \"Avg Local\"]\n",
    "#labels = names\n",
    "\n",
    "xscales = [1, 1, 1, 1, 1, 1]\n",
    "#xscales = [1, 1, 1, 1, 1, 1]\n",
    "#xscales = [1, 1, 1, 1, 1, 1]\n",
    "\n",
    "metrics = val_losses\n",
    "stds = val_std\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for metric, std, xscale, label, color in zip(metrics, stds, xscales, labels, colors):\n",
    "    plt.plot([20, 100, 200, 300, 400, 500, 1000], metric, label=label, color=color, marker='o')\n",
    "    plt.fill_between([20, 100, 200, 300, 400, 500, 1000], metric-std, metric+std, color=color, alpha=0.2)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Amt of Data at Each Site\", fontsize=14)\n",
    "plt.ylabel(\"Loss (Mean Squared Error)\", fontsize=14)\n",
    "plt.title(\"Minimum Validation Loss over Site Sample Size\\n (Synthetic Data)\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71af6fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Testing...\")\n",
    "\n",
    "hypers = {\"max_rounds\": 1, \"max_iter\": 100, \"batch_size\": None, \"lr\": 0.001, \"momentum\": 0.9, \"steps\": 10, \"threshold\": 0.0}\n",
    "aggregator_fn = logistic_loss\n",
    "lsr_dot_params = (shape, (3, 3, 3), 2, torch.float64, torch.device('cuda'))\n",
    "\n",
    "init_lsr_dot = LSR_tensor_dot(*lsr_dot_params)\n",
    "lsr_bcd_regression(init_lsr_dot, loss_fn, vessel_dataset, vessel_val_dataset,\\\n",
    "                       hypers, True, True)\n",
    "\n",
    "print(\"Finished without errors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de758f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance testing\n",
    "print(\"Performance Testing...\")\n",
    "hypers = {\"max_rounds\": 1, \"max_iter\": 100, \"batch_size\": None, \"lr\": 0.005, \"momentum\": 0.9, \"steps\": 10, \"threshold\": 0.0}\n",
    "aggregator_fn = avg_aggregation\n",
    "lsr_dot_params = (shape, ranks, separation_rank, torch.float32, torch.device('cuda'))\n",
    "init_lsr_dot = LSR_tensor_dot(*lsr_dot_params)\n",
    "cProfile.run(\"BCD_federated_stepwise(init_lsr_dot, synth_client_datasets, synth_val_dataset,\\\n",
    "              hypers, loss_fn, aggregator_fn, False)\", sort='tottime')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
